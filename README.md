# Project Overview
This project aims to enhance the efficiency of state-space models (SSMs) and vision-language models (VLMs). The project is divided into two main parts:

## 1. Vision Mamba Efficiency Improvement
The first part focuses on improving the efficiency of the Vision Mamba model. The goal is to calculate the FLOPs (Floating Point Operations) during training with full images and compare it with training using images that have certain patches dropped.

## 2. OpenAI CLIP-L (ViT-L/14) Efficiency Improvement
The second part of the project is focused on enhancing the efficiency of VILA, a vision-language model, specifically [OpenAI CLIP-L(ViT-L/14)] (https://huggingface.co/openai/clip-vit-large-patch14). The objective is to optimize the training process and improve both the computational efficiency and model performance.

**Overview of Folders:**  
* ```Part_1_and_2```: Contains the code, utility files, and the Vision Mamba block used to improve the training efficiency on the [Cats vs Dogs - 2000 images (224x224)] (https://www.kaggle.com/datasets/abhinavnayak/catsvdogs-transformed/data).
* ```Part_3_and_4```: Contains the code applied to the [Kaggle Clothes Image Classifier] (https://www.kaggle.com/code/marissafernandes/clothes-image-classifier/notebook) dataset to improve the efficiency of [OpenAI CLIP-L(ViT-L/14)] (https://huggingface.co/openai/clip-vit-large-patch14).
  
**Workflow for Vision Mamba Efficiency Improvement:**

(Insert Workflow Diagram Here)
**Workflow for OpenAI CLIP-L (ViT-L/14) Efficiency Improvement:**
(Insert Workflow Diagram Here)


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2VVUwSE6qWX",
    "outputId": "6d661732-4ead-4534-92b1-f0c8ac8df262"
   },
   "source": [
    "## Effieciecny and accuracy improvement "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This code is run on GPU. This code is build to make the testing process faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWa1BrjW6w5E",
    "outputId": "45305fe1-6b8d-4906-fdff-3059bc666776"
   },
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/ece_project2\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWuS9GwL6P0J",
    "outputId": "d80488eb-0461-4b34-ad40-3ee3c77ca95a"
   },
   "outputs": [],
   "source": [
    "# #!/bin/bash\n",
    "# !kaggle datasets download abhinavnayak/catsvdogs-transformed -p \"/content/drive/MyDrive/ece_project2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWuGalqB7cZ9"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('/content/drive/MyDrive/ece_project2/catsvdogs-transformed.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/drive/MyDrive/ece_project2/cats-dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRbnXZfS79He",
    "outputId": "07c592e0-c7f5-4478-9e78-1606fdf50dd1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "base_dir = \"cats-dogs/train_transformed\"\n",
    "output_dir = \"dataset\"\n",
    "\n",
    "cats_dir = os.path.join(output_dir, \"cats\")\n",
    "dogs_dir = os.path.join(output_dir, \"dogs\")\n",
    "\n",
    "os.makedirs(cats_dir, exist_ok=True)\n",
    "os.makedirs(dogs_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for filename in os.listdir(base_dir):\n",
    "    if filename.lower().startswith(\"cat\"):\n",
    "        shutil.move(os.path.join(base_dir, filename), os.path.join(cats_dir, filename))\n",
    "    elif filename.lower().startswith(\"dog\"):\n",
    "        shutil.move(os.path.join(base_dir, filename), os.path.join(dogs_dir, filename))\n",
    "\n",
    "print(\"Files have been successfully sorted into 'cats' and 'dogs' folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Keyrq2HMAEPX"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "dest_dataset_dir = \"dataset\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root=dest_dataset_dir, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Vision Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVptcPLIA5Hn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "class SimplifiedMamba(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2, device=None, dtype=None):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(expand * d_model)\n",
    "\n",
    "        # Input projection\n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, **factory_kwargs)\n",
    "\n",
    "        # Convolution for local mixing\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            kernel_size=d_conv,\n",
    "            groups=self.d_inner,\n",
    "            padding=d_conv - 1,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, **factory_kwargs)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        hidden_states: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch, seqlen, dim = hidden_states.shape\n",
    "\n",
    "        xz = self.in_proj(hidden_states)\n",
    "        x, z = xz.chunk(2, dim=-1)\n",
    "\n",
    "        x = self.conv1d(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if x.shape[1] != z.shape[1]:\n",
    "            x = x[:, :z.shape[1], :]\n",
    "\n",
    "        out = self.out_proj(x * z)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wa3UQBtc40hD",
    "outputId": "36f62817-2cd4-465e-9184-7f4325f0be8d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "\n",
    "class SimplifiedVisionMamba(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        stride=16,\n",
    "        depth=12,\n",
    "        embed_dim=192,\n",
    "        num_classes=1000,\n",
    "        d_state=16,\n",
    "        drop_rate=0.1,\n",
    "        drop_path_rate=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            stride=stride,\n",
    "            in_chans=3,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # Positional Embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    mixer_cls=partial(SimplifiedMamba, d_state=d_state),\n",
    "                    norm_cls=nn.LayerNorm,\n",
    "                    drop_path=dpr[i],\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        # Final Classifier Head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize Weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.ones_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Patch Embedding\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Add Positional Embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # Transformer Layers\n",
    "        for layer in self.layers:\n",
    "            x, _ = layer(x)\n",
    "\n",
    "        # Classification Head\n",
    "        x = self.norm(x[:, 0])  # CLS token output\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Simple 2D Patch Embedding\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, stride=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (\n",
    "            (img_size - patch_size) // stride + 1,\n",
    "            (img_size - patch_size) // stride + 1,\n",
    "        )\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=stride\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # Convert to patches\n",
    "        x = x.flatten(2).transpose(1, 2)  # Flatten into patch tokens\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block with Mamba Mixer\"\"\"\n",
    "    def __init__(self, dim, mixer_cls, norm_cls=nn.LayerNorm, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.mixer = mixer_cls(dim)\n",
    "        self.norm1 = norm_cls(dim)\n",
    "        self.norm2 = norm_cls(dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x, residual=None):\n",
    "        # Self-attention with residual\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.mixer(x)\n",
    "        x = residual + self.drop_path(x)  # Add & Norm\n",
    "\n",
    "        # MLP with residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        return x, residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "103xS11FHeZ5",
    "outputId": "0aa39da8-30cb-4575-b2b8-47971990aa1b"
   },
   "outputs": [],
   "source": [
    "# !pip install fvcore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full patches train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qvxyxz3H_yIW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device, num_epochs=20):\n",
    "    total_flops = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            if total_flops == 0:\n",
    "                flop_analyzer = FlopCountAnalysis(model, images)\n",
    "                total_flops = flop_analyzer.total()\n",
    "                print(f\"FLOPs per forward pass: {total_flops} FLOPs\")\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * train_correct / len(train_loader.dataset)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}: Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "def test(model, test_loader, criterion, device, num_iterations=10):\n",
    "    total_test_loss = 0\n",
    "    total_test_accuracy = 0\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                test_correct += (preds == labels).sum().item()\n",
    "\n",
    "        test_accuracy = 100 * test_correct / len(test_loader.dataset)\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}: Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        total_test_loss += avg_test_loss\n",
    "        total_test_accuracy += test_accuracy\n",
    "\n",
    "    avg_loss = total_test_loss / num_iterations\n",
    "    avg_accuracy = total_test_accuracy / num_iterations\n",
    "    print(f\"\\nAverage Test Loss over {num_iterations} runs: {avg_loss:.4f}\")\n",
    "    print(f\"Average Test Accuracy over {num_iterations} runs: {avg_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCMGCqe-B2xi",
    "outputId": "8e8950b3-069f-4b58-f70c-3a20c761d0aa"
   },
   "outputs": [],
   "source": [
    "model = SimplifiedVisionMamba(num_classes=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, device, num_epochs=30)\n",
    "\n",
    "test(model, test_loader, criterion, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop 50% patches trin and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_wQBZWUEk-k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "\n",
    "class SimplifiedVisionMambaWithDrop(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        stride=16,\n",
    "        depth=12,\n",
    "        embed_dim=192,\n",
    "        num_classes=1000,\n",
    "        d_state=16,\n",
    "        drop_rate=0.1,\n",
    "        drop_path_rate=0.1,\n",
    "        drop_patch_prob=0.8,  # Probability to drop patches\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Patch Embedding with patch dropping\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            stride=stride,\n",
    "            in_chans=3,\n",
    "            embed_dim=embed_dim,\n",
    "            drop_patch_prob=drop_patch_prob,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # Positional Embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    mixer_cls=partial(SimplifiedMamba, d_state=d_state),\n",
    "                    norm_cls=nn.LayerNorm,\n",
    "                    drop_path=dpr[i],\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final Classifier Head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize Weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.ones_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Patch Embedding\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Add Positional Embeddings\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]  # Adjust for dropped patches\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # Transformer Layers\n",
    "        for layer in self.layers:\n",
    "            x, _ = layer(x)\n",
    "\n",
    "        # Classification Head\n",
    "        x = self.norm(x[:, 0])  # CLS token output\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"2D Patch Embedding with Patch Dropping\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, stride=16, in_chans=3, embed_dim=768, drop_patch_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (\n",
    "            (img_size - patch_size) // stride + 1,\n",
    "            (img_size - patch_size) // stride + 1,\n",
    "        )\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.drop_patch_prob = drop_patch_prob\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=stride\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # Convert to patches\n",
    "        x = x.flatten(2).transpose(1, 2)  # Flatten into patch tokens [B, num_patches, embed_dim]\n",
    "\n",
    "        # Drop patches probabilistically\n",
    "        if self.drop_patch_prob > 0.0 and self.training:\n",
    "            batch_size, num_patches, _ = x.size()\n",
    "            patch_mask = torch.rand(num_patches, device=x.device) > self.drop_patch_prob\n",
    "            x = x[:, patch_mask, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block with Mamba Mixer\"\"\"\n",
    "    def __init__(self, dim, mixer_cls, norm_cls=nn.LayerNorm, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.mixer = mixer_cls(dim)\n",
    "        self.norm1 = norm_cls(dim)\n",
    "        self.norm2 = norm_cls(dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x, residual=None):\n",
    "        # Self-attention with residual\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.mixer(x)\n",
    "        x = residual + self.drop_path(x)  # Add & Norm\n",
    "\n",
    "        # MLP with residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        return x, residual\n",
    "\n",
    "\n",
    "class SimplifiedMamba(nn.Module):\n",
    "    \"\"\"A simplified version of Mamba Mixer.\"\"\"\n",
    "    def __init__(self, dim, d_state=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, d_state)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_state, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1Vxf3RuFxP4",
    "outputId": "36179e31-d63e-44a2-aff7-89c994a380d7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SimplifiedVisionMambaWithDrop(num_classes=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, device, num_epochs=30)\n",
    "\n",
    "test(model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop 80% patches trin and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZ7XDqBbI-Ec"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "\n",
    "class SimplifiedVisionMambaWith80Drop(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        stride=16,\n",
    "        depth=12,\n",
    "        embed_dim=192,\n",
    "        num_classes=1000,\n",
    "        d_state=16,\n",
    "        drop_rate=0.1,\n",
    "        drop_path_rate=0.1,\n",
    "        drop_patch_prob=0.8,  # Probability to drop patches\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Patch Embedding with patch dropping\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            stride=stride,\n",
    "            in_chans=3,\n",
    "            embed_dim=embed_dim,\n",
    "            drop_patch_prob=drop_patch_prob,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # Positional Embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    mixer_cls=partial(SimplifiedMamba, d_state=d_state),\n",
    "                    norm_cls=nn.LayerNorm,\n",
    "                    drop_path=dpr[i],\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final Classifier Head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize Weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.ones_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Patch Embedding\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Add Positional Embeddings\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]  # Adjust for dropped patches\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # Transformer Layers\n",
    "        for layer in self.layers:\n",
    "            x, _ = layer(x)\n",
    "\n",
    "        # Classification Head\n",
    "        x = self.norm(x[:, 0])  # CLS token output\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"2D Patch Embedding with Patch Dropping\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, stride=16, in_chans=3, embed_dim=768, drop_patch_prob=0.8):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (\n",
    "            (img_size - patch_size) // stride + 1,\n",
    "            (img_size - patch_size) // stride + 1,\n",
    "        )\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.drop_patch_prob = drop_patch_prob\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=stride\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # Convert to patches\n",
    "        x = x.flatten(2).transpose(1, 2)  # Flatten into patch tokens [B, num_patches, embed_dim]\n",
    "\n",
    "        # Drop patches probabilistically\n",
    "        if self.drop_patch_prob > 0.0 and self.training:\n",
    "            batch_size, num_patches, _ = x.size()\n",
    "            patch_mask = torch.rand(num_patches, device=x.device) > self.drop_patch_prob\n",
    "            x = x[:, patch_mask, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block with Mamba Mixer\"\"\"\n",
    "    def __init__(self, dim, mixer_cls, norm_cls=nn.LayerNorm, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.mixer = mixer_cls(dim)\n",
    "        self.norm1 = norm_cls(dim)\n",
    "        self.norm2 = norm_cls(dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x, residual=None):\n",
    "        # Self-attention with residual\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.mixer(x)\n",
    "        x = residual + self.drop_path(x)  # Add & Norm\n",
    "\n",
    "        # MLP with residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        return x, residual\n",
    "\n",
    "\n",
    "class SimplifiedMamba(nn.Module):\n",
    "    \"\"\"A simplified version of Mamba Mixer.\"\"\"\n",
    "    def __init__(self, dim, d_state=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, d_state)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_state, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0msTELOHJj51",
    "outputId": "77853f87-a0a0-457a-ee6c-91a75c72afa0"
   },
   "outputs": [],
   "source": [
    "model = SimplifiedVisionMambaWith80Drop(num_classes=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, device, num_epochs=30)\n",
    "\n",
    "test(model, test_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

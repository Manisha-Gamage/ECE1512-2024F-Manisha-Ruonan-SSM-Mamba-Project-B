{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Effieciecny and accuracy improvement\n",
        "This code is run on GPU. This code is build to make the testing process faster"
      ],
      "metadata": {
        "id": "EVI8YH-E_7jN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2VVUwSE6qWX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWa1BrjW6w5E"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive/ece_project2\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AWuS9GwL6P0J"
      },
      "outputs": [],
      "source": [
        "# #!/bin/bash\n",
        "# !kaggle datasets download abhinavnayak/catsvdogs-transformed -p \"/content/drive/MyDrive/ece_project2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jWuGalqB7cZ9"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# with zipfile.ZipFile('/content/drive/MyDrive/ece_project2/catsvdogs-transformed.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('/content/drive/MyDrive/ece_project2/cats-dogs')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "nz0JlnMr00ve"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BRbnXZfS79He"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "base_dir = \"cats-dogs/train_transformed\"\n",
        "output_dir = \"dataset\"\n",
        "\n",
        "cats_dir = os.path.join(output_dir, \"cats\")\n",
        "dogs_dir = os.path.join(output_dir, \"dogs\")\n",
        "\n",
        "os.makedirs(cats_dir, exist_ok=True)\n",
        "os.makedirs(dogs_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "for filename in os.listdir(base_dir):\n",
        "    if filename.lower().startswith(\"cat\"):\n",
        "        shutil.move(os.path.join(base_dir, filename), os.path.join(cats_dir, filename))\n",
        "    elif filename.lower().startswith(\"dog\"):\n",
        "        shutil.move(os.path.join(base_dir, filename), os.path.join(dogs_dir, filename))\n",
        "\n",
        "print(\"Files have been successfully sorted into 'cats' and 'dogs' folders.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Keyrq2HMAEPX"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "dest_dataset_dir = \"dataset\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "dataset = ImageFolder(root=dest_dataset_dir, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Vision Mamba"
      ],
      "metadata": {
        "id": "_j35lA1f1BoF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kVptcPLIA5Hn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "class SimplifiedMamba(nn.Module):\n",
        "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2, device=None, dtype=None):\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.expand = expand\n",
        "        self.d_inner = int(expand * d_model)\n",
        "\n",
        "        # Input projection\n",
        "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, **factory_kwargs)\n",
        "\n",
        "        # Convolution for local mixing\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=self.d_inner,\n",
        "            out_channels=self.d_inner,\n",
        "            kernel_size=d_conv,\n",
        "            groups=self.d_inner,\n",
        "            padding=d_conv - 1,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "        self.activation = nn.SiLU()\n",
        "\n",
        "        # Output projection\n",
        "        self.out_proj = nn.Linear(self.d_inner, d_model, **factory_kwargs)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \"\"\"\n",
        "        hidden_states: (batch, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch, seqlen, dim = hidden_states.shape\n",
        "\n",
        "        xz = self.in_proj(hidden_states)\n",
        "        x, z = xz.chunk(2, dim=-1)\n",
        "\n",
        "        x = self.conv1d(x.transpose(1, 2)).transpose(1, 2)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        if x.shape[1] != z.shape[1]:\n",
        "            x = x[:, :z.shape[1], :]\n",
        "\n",
        "        out = self.out_proj(x * z)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa3UQBtc40hD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "\n",
        "\n",
        "class SimplifiedVisionMamba(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        stride=16,\n",
        "        depth=12,\n",
        "        embed_dim=192,\n",
        "        num_classes=1000,\n",
        "        d_state=16,\n",
        "        drop_rate=0.1,\n",
        "        drop_path_rate=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Patch Embedding\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            stride=stride,\n",
        "            in_chans=3,\n",
        "            embed_dim=embed_dim,\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Positional Embeddings\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # Transformer Blocks\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    dim=embed_dim,\n",
        "                    mixer_cls=partial(SimplifiedMamba, d_state=d_state),\n",
        "                    norm_cls=nn.LayerNorm,\n",
        "                    drop_path=dpr[i],\n",
        "                )\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "        # Final Classifier Head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize Weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.ones_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch Embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add Positional Embeddings\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Transformer Layers\n",
        "        for layer in self.layers:\n",
        "            x, _ = layer(x)\n",
        "\n",
        "        # Classification Head\n",
        "        x = self.norm(x[:, 0])  # CLS token output\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Simple 2D Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, stride=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (\n",
        "            (img_size - patch_size) // stride + 1,\n",
        "            (img_size - patch_size) // stride + 1,\n",
        "        )\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans, embed_dim, kernel_size=patch_size, stride=stride\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # Convert to patches\n",
        "        x = x.flatten(2).transpose(1, 2)  # Flatten into patch tokens\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer Block with Mamba Mixer\"\"\"\n",
        "    def __init__(self, dim, mixer_cls, norm_cls=nn.LayerNorm, drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.mixer = mixer_cls(dim)\n",
        "        self.norm1 = norm_cls(dim)\n",
        "        self.norm2 = norm_cls(dim)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        # Self-attention with residual\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.mixer(x)\n",
        "        x = residual + self.drop_path(x)  # Add & Norm\n",
        "\n",
        "        # MLP with residual\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        return x, residual\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "103xS11FHeZ5"
      },
      "outputs": [],
      "source": [
        "# !pip install fvcore"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and evaluation"
      ],
      "metadata": {
        "id": "p9l2eQBE1NCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Full patches train and test"
      ],
      "metadata": {
        "id": "u6uRBwwq1Q89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Qvxyxz3H_yIW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device, num_epochs=20):\n",
        "    total_flops = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            if total_flops == 0:\n",
        "                flop_analyzer = FlopCountAnalysis(model, images)\n",
        "                total_flops = flop_analyzer.total()\n",
        "                print(f\"FLOPs per forward pass: {total_flops} FLOPs\")\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * train_correct / len(train_loader.dataset)\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}: Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def test(model, test_loader, criterion, device, num_iterations=10):\n",
        "    total_test_loss = 0\n",
        "    total_test_accuracy = 0\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        test_correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                test_correct += (preds == labels).sum().item()\n",
        "\n",
        "        test_accuracy = 100 * test_correct / len(test_loader.dataset)\n",
        "        avg_test_loss = test_loss / len(test_loader)\n",
        "\n",
        "        print(f\"Iteration {iteration + 1}: Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "        total_test_loss += avg_test_loss\n",
        "        total_test_accuracy += test_accuracy\n",
        "\n",
        "    avg_loss = total_test_loss / num_iterations\n",
        "    avg_accuracy = total_test_accuracy / num_iterations\n",
        "    print(f\"\\nAverage Test Loss over {num_iterations} runs: {avg_loss:.4f}\")\n",
        "    print(f\"Average Test Accuracy over {num_iterations} runs: {avg_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCMGCqe-B2xi",
        "outputId": "ff29d20a-2ac9-44fe-8972-8114e9da0010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 13 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::silu encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 23 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::bernoulli_ encountered 11 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div_ encountered 11 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs per forward pass: 17833961472 FLOPs\n",
            "Epoch 1/35: Train Loss: 0.6925, Train Accuracy: 53.14%\n",
            "Epoch 2/35: Train Loss: 0.6923, Train Accuracy: 54.23%\n",
            "Epoch 3/35: Train Loss: 0.6756, Train Accuracy: 55.92%\n",
            "Epoch 4/35: Train Loss: 0.6738, Train Accuracy: 57.74%\n",
            "Epoch 5/35: Train Loss: 0.6666, Train Accuracy: 57.54%\n",
            "Epoch 6/35: Train Loss: 0.6625, Train Accuracy: 59.97%\n",
            "Epoch 7/35: Train Loss: 0.6616, Train Accuracy: 57.20%\n",
            "Epoch 8/35: Train Loss: 0.6547, Train Accuracy: 58.55%\n",
            "Epoch 9/35: Train Loss: 0.6680, Train Accuracy: 55.92%\n",
            "Epoch 10/35: Train Loss: 0.6433, Train Accuracy: 61.46%\n",
            "Epoch 11/35: Train Loss: 0.6407, Train Accuracy: 60.45%\n",
            "Epoch 12/35: Train Loss: 0.6364, Train Accuracy: 60.11%\n",
            "Epoch 13/35: Train Loss: 0.6282, Train Accuracy: 62.34%\n",
            "Epoch 14/35: Train Loss: 0.6347, Train Accuracy: 61.60%\n",
            "Epoch 15/35: Train Loss: 0.6227, Train Accuracy: 63.08%\n",
            "Epoch 16/35: Train Loss: 0.6121, Train Accuracy: 64.77%\n",
            "Epoch 17/35: Train Loss: 0.5998, Train Accuracy: 63.35%\n",
            "Epoch 18/35: Train Loss: 0.5917, Train Accuracy: 64.91%\n",
            "Epoch 19/35: Train Loss: 0.5946, Train Accuracy: 64.77%\n",
            "Epoch 20/35: Train Loss: 0.5806, Train Accuracy: 65.31%\n",
            "Epoch 21/35: Train Loss: 0.5833, Train Accuracy: 65.99%\n",
            "Epoch 22/35: Train Loss: 0.5792, Train Accuracy: 64.50%\n",
            "Epoch 23/35: Train Loss: 0.5671, Train Accuracy: 66.87%\n",
            "Epoch 24/35: Train Loss: 0.5643, Train Accuracy: 69.30%\n",
            "Epoch 25/35: Train Loss: 0.5649, Train Accuracy: 67.68%\n",
            "Epoch 26/35: Train Loss: 0.5442, Train Accuracy: 68.83%\n",
            "Epoch 27/35: Train Loss: 0.5624, Train Accuracy: 66.46%\n",
            "Epoch 28/35: Train Loss: 0.5426, Train Accuracy: 68.63%\n",
            "Epoch 29/35: Train Loss: 0.5349, Train Accuracy: 68.42%\n",
            "Epoch 30/35: Train Loss: 0.5281, Train Accuracy: 70.45%\n",
            "Epoch 31/35: Train Loss: 0.5334, Train Accuracy: 67.75%\n",
            "Epoch 32/35: Train Loss: 0.5243, Train Accuracy: 69.98%\n",
            "Epoch 33/35: Train Loss: 0.5161, Train Accuracy: 69.71%\n",
            "Epoch 34/35: Train Loss: 0.5044, Train Accuracy: 70.99%\n",
            "Epoch 35/35: Train Loss: 0.5181, Train Accuracy: 70.25%\n",
            "Training complete!\n",
            "Iteration 1: Test Loss: 0.7870, Test Accuracy: 54.59%\n",
            "Iteration 2: Test Loss: 0.7870, Test Accuracy: 54.59%\n",
            "Iteration 3: Test Loss: 0.7870, Test Accuracy: 54.59%\n",
            "Iteration 4: Test Loss: 0.7870, Test Accuracy: 54.59%\n",
            "Iteration 5: Test Loss: 0.7870, Test Accuracy: 54.59%\n",
            "Iteration 6: Test Loss: 0.7870, Test Accuracy: 54.59%\n",
            "Iteration 7: Test Loss: 0.7870, Test Accuracy: 54.59%\n",
            "Iteration 8: Test Loss: 0.7870, Test Accuracy: 54.59%\n",
            "Iteration 9: Test Loss: 0.7870, Test Accuracy: 54.59%\n",
            "Iteration 10: Test Loss: 0.7870, Test Accuracy: 54.59%\n",
            "\n",
            "Average Test Loss over 10 runs: 0.7870\n",
            "Average Test Accuracy over 10 runs: 54.59%\n"
          ]
        }
      ],
      "source": [
        "model = SimplifiedVisionMamba(num_classes=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, device, num_epochs=35)\n",
        "\n",
        "test(model, test_loader, criterion, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop 50% patches train and test"
      ],
      "metadata": {
        "id": "JqwJFi6M1XIU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0_wQBZWUEk-k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "\n",
        "\n",
        "class SimplifiedVisionMambaWithDrop(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        stride=16,\n",
        "        depth=12,\n",
        "        embed_dim=192,\n",
        "        num_classes=1000,\n",
        "        d_state=16,\n",
        "        drop_rate=0.1,\n",
        "        drop_path_rate=0.1,\n",
        "        drop_patch_prob=0.5,  # Probability to drop patches\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Patch Embedding with patch dropping\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            stride=stride,\n",
        "            in_chans=3,\n",
        "            embed_dim=embed_dim,\n",
        "            drop_patch_prob=drop_patch_prob,\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Positional Embeddings\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # Transformer Blocks\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    dim=embed_dim,\n",
        "                    mixer_cls=partial(SimplifiedMamba, d_state=d_state),\n",
        "                    norm_cls=nn.LayerNorm,\n",
        "                    drop_path=dpr[i],\n",
        "                )\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Final Classifier Head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize Weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.ones_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch Embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add Positional Embeddings\n",
        "        x = x + self.pos_embed[:, :x.size(1), :]  # Adjust for dropped patches\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Transformer Layers\n",
        "        for layer in self.layers:\n",
        "            x, _ = layer(x)\n",
        "\n",
        "        # Classification Head\n",
        "        x = self.norm(x[:, 0])  # CLS token output\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"2D Patch Embedding with Patch Dropping\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, stride=16, in_chans=3, embed_dim=768, drop_patch_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (\n",
        "            (img_size - patch_size) // stride + 1,\n",
        "            (img_size - patch_size) // stride + 1,\n",
        "        )\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "        self.drop_patch_prob = drop_patch_prob\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans, embed_dim, kernel_size=patch_size, stride=stride\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # Convert to patches\n",
        "        x = x.flatten(2).transpose(1, 2)  # Flatten into patch tokens [B, num_patches, embed_dim]\n",
        "\n",
        "        # Drop patches probabilistically\n",
        "        if self.drop_patch_prob > 0.0 and self.training:\n",
        "            batch_size, num_patches, _ = x.size()\n",
        "            patch_mask = torch.rand(num_patches, device=x.device) > self.drop_patch_prob\n",
        "            x = x[:, patch_mask, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer Block with Mamba Mixer\"\"\"\n",
        "    def __init__(self, dim, mixer_cls, norm_cls=nn.LayerNorm, drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.mixer = mixer_cls(dim)\n",
        "        self.norm1 = norm_cls(dim)\n",
        "        self.norm2 = norm_cls(dim)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        # Self-attention with residual\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.mixer(x)\n",
        "        x = residual + self.drop_path(x)  # Add & Norm\n",
        "\n",
        "        # MLP with residual\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        return x, residual\n",
        "\n",
        "\n",
        "class SimplifiedMamba(nn.Module):\n",
        "    \"\"\"A simplified version of Mamba Mixer.\"\"\"\n",
        "    def __init__(self, dim, d_state=16):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, d_state)\n",
        "        self.act = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(d_state, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.act(self.fc1(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Vxf3RuFxP4",
        "outputId": "8986218d-b73c-477a-9075-0594428e6cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::rand encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 13 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::bernoulli_ encountered 11 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div_ encountered 11 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 11 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs per forward pass: 1215965184 FLOPs\n",
            "Epoch 1/35: Train Loss: 0.6999, Train Accuracy: 51.66%\n",
            "Epoch 2/35: Train Loss: 0.6867, Train Accuracy: 54.16%\n",
            "Epoch 3/35: Train Loss: 0.6882, Train Accuracy: 55.10%\n",
            "Epoch 4/35: Train Loss: 0.6810, Train Accuracy: 54.16%\n",
            "Epoch 5/35: Train Loss: 0.6850, Train Accuracy: 54.83%\n",
            "Epoch 6/35: Train Loss: 0.6815, Train Accuracy: 55.58%\n",
            "Epoch 7/35: Train Loss: 0.6799, Train Accuracy: 54.90%\n",
            "Epoch 8/35: Train Loss: 0.6767, Train Accuracy: 55.17%\n",
            "Epoch 9/35: Train Loss: 0.6766, Train Accuracy: 54.50%\n",
            "Epoch 10/35: Train Loss: 0.6756, Train Accuracy: 56.19%\n",
            "Epoch 11/35: Train Loss: 0.6762, Train Accuracy: 57.94%\n",
            "Epoch 12/35: Train Loss: 0.6740, Train Accuracy: 56.05%\n",
            "Epoch 13/35: Train Loss: 0.6671, Train Accuracy: 56.32%\n",
            "Epoch 14/35: Train Loss: 0.6696, Train Accuracy: 57.40%\n",
            "Epoch 15/35: Train Loss: 0.6737, Train Accuracy: 56.39%\n",
            "Epoch 16/35: Train Loss: 0.6656, Train Accuracy: 58.08%\n",
            "Epoch 17/35: Train Loss: 0.6723, Train Accuracy: 56.86%\n",
            "Epoch 18/35: Train Loss: 0.6666, Train Accuracy: 57.20%\n",
            "Epoch 19/35: Train Loss: 0.6670, Train Accuracy: 57.54%\n",
            "Epoch 20/35: Train Loss: 0.6730, Train Accuracy: 56.19%\n",
            "Epoch 21/35: Train Loss: 0.6720, Train Accuracy: 56.86%\n",
            "Epoch 22/35: Train Loss: 0.6684, Train Accuracy: 58.01%\n",
            "Epoch 23/35: Train Loss: 0.6585, Train Accuracy: 58.08%\n",
            "Epoch 24/35: Train Loss: 0.6550, Train Accuracy: 59.43%\n",
            "Epoch 25/35: Train Loss: 0.6605, Train Accuracy: 57.81%\n",
            "Epoch 26/35: Train Loss: 0.6654, Train Accuracy: 56.80%\n",
            "Epoch 27/35: Train Loss: 0.6601, Train Accuracy: 56.93%\n",
            "Epoch 28/35: Train Loss: 0.6631, Train Accuracy: 59.03%\n",
            "Epoch 29/35: Train Loss: 0.6563, Train Accuracy: 58.55%\n",
            "Epoch 30/35: Train Loss: 0.6542, Train Accuracy: 58.15%\n",
            "Epoch 31/35: Train Loss: 0.6592, Train Accuracy: 60.31%\n",
            "Epoch 32/35: Train Loss: 0.6625, Train Accuracy: 55.92%\n",
            "Epoch 33/35: Train Loss: 0.6548, Train Accuracy: 59.70%\n",
            "Epoch 34/35: Train Loss: 0.6500, Train Accuracy: 59.23%\n",
            "Epoch 35/35: Train Loss: 0.6524, Train Accuracy: 58.76%\n",
            "Training complete!\n",
            "Iteration 1: Test Loss: 0.6605, Test Accuracy: 60.54%\n",
            "Iteration 2: Test Loss: 0.6605, Test Accuracy: 60.54%\n",
            "Iteration 3: Test Loss: 0.6605, Test Accuracy: 60.54%\n",
            "Iteration 4: Test Loss: 0.6605, Test Accuracy: 60.54%\n",
            "Iteration 5: Test Loss: 0.6605, Test Accuracy: 60.54%\n",
            "Iteration 6: Test Loss: 0.6605, Test Accuracy: 60.54%\n",
            "Iteration 7: Test Loss: 0.6605, Test Accuracy: 60.54%\n",
            "Iteration 8: Test Loss: 0.6605, Test Accuracy: 60.54%\n",
            "Iteration 9: Test Loss: 0.6605, Test Accuracy: 60.54%\n",
            "Iteration 10: Test Loss: 0.6605, Test Accuracy: 60.54%\n",
            "\n",
            "Average Test Loss over 10 runs: 0.6605\n",
            "Average Test Accuracy over 10 runs: 60.54%\n"
          ]
        }
      ],
      "source": [
        "model = SimplifiedVisionMambaWithDrop(num_classes=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, device, num_epochs=35)\n",
        "\n",
        "test(model, test_loader, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop 80% patches train and test"
      ],
      "metadata": {
        "id": "AhpVFYJ-1htL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vZ7XDqBbI-Ec"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "\n",
        "\n",
        "class SimplifiedVisionMambaWith80Drop(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        stride=16,\n",
        "        depth=12,\n",
        "        embed_dim=192,\n",
        "        num_classes=1000,\n",
        "        d_state=16,\n",
        "        drop_rate=0.1,\n",
        "        drop_path_rate=0.1,\n",
        "        drop_patch_prob=0.8,  # Probability to drop patches\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Patch Embedding with patch dropping\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            stride=stride,\n",
        "            in_chans=3,\n",
        "            embed_dim=embed_dim,\n",
        "            drop_patch_prob=drop_patch_prob,\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Positional Embeddings\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # Transformer Blocks\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    dim=embed_dim,\n",
        "                    mixer_cls=partial(SimplifiedMamba, d_state=d_state),\n",
        "                    norm_cls=nn.LayerNorm,\n",
        "                    drop_path=dpr[i],\n",
        "                )\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Final Classifier Head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize Weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.ones_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch Embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add Positional Embeddings\n",
        "        x = x + self.pos_embed[:, :x.size(1), :]  # Adjust for dropped patches\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Transformer Layers\n",
        "        for layer in self.layers:\n",
        "            x, _ = layer(x)\n",
        "\n",
        "        # Classification Head\n",
        "        x = self.norm(x[:, 0])  # CLS token output\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"2D Patch Embedding with Patch Dropping\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, stride=16, in_chans=3, embed_dim=768, drop_patch_prob=0.8):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (\n",
        "            (img_size - patch_size) // stride + 1,\n",
        "            (img_size - patch_size) // stride + 1,\n",
        "        )\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "        self.drop_patch_prob = drop_patch_prob\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans, embed_dim, kernel_size=patch_size, stride=stride\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # Convert to patches\n",
        "        x = x.flatten(2).transpose(1, 2)  # Flatten into patch tokens [B, num_patches, embed_dim]\n",
        "\n",
        "        # Drop patches probabilistically\n",
        "        if self.drop_patch_prob > 0.0 and self.training:\n",
        "            batch_size, num_patches, _ = x.size()\n",
        "            patch_mask = torch.rand(num_patches, device=x.device) > self.drop_patch_prob\n",
        "            x = x[:, patch_mask, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer Block with Mamba Mixer\"\"\"\n",
        "    def __init__(self, dim, mixer_cls, norm_cls=nn.LayerNorm, drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.mixer = mixer_cls(dim)\n",
        "        self.norm1 = norm_cls(dim)\n",
        "        self.norm2 = norm_cls(dim)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        # Self-attention with residual\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.mixer(x)\n",
        "        x = residual + self.drop_path(x)  # Add & Norm\n",
        "\n",
        "        # MLP with residual\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        return x, residual\n",
        "\n",
        "\n",
        "class SimplifiedMamba(nn.Module):\n",
        "    \"\"\"A simplified version of Mamba Mixer.\"\"\"\n",
        "    def __init__(self, dim, d_state=16):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, d_state)\n",
        "        self.act = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(d_state, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.act(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0msTELOHJj51",
        "outputId": "142ed983-ffc0-4892-9b8d-9c86275882e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::rand encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 13 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::bernoulli_ encountered 11 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div_ encountered 11 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 11 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs per forward pass: 1042556928 FLOPs\n",
            "Epoch 1/35: Train Loss: 0.7019, Train Accuracy: 49.63%\n",
            "Epoch 2/35: Train Loss: 0.6894, Train Accuracy: 53.21%\n",
            "Epoch 3/35: Train Loss: 0.6939, Train Accuracy: 51.52%\n",
            "Epoch 4/35: Train Loss: 0.6914, Train Accuracy: 53.21%\n",
            "Epoch 5/35: Train Loss: 0.6878, Train Accuracy: 52.60%\n",
            "Epoch 6/35: Train Loss: 0.6784, Train Accuracy: 54.63%\n",
            "Epoch 7/35: Train Loss: 0.6849, Train Accuracy: 53.89%\n",
            "Epoch 8/35: Train Loss: 0.6897, Train Accuracy: 53.62%\n",
            "Epoch 9/35: Train Loss: 0.6819, Train Accuracy: 53.21%\n",
            "Epoch 10/35: Train Loss: 0.6868, Train Accuracy: 51.05%\n",
            "Epoch 11/35: Train Loss: 0.6838, Train Accuracy: 54.02%\n",
            "Epoch 12/35: Train Loss: 0.6888, Train Accuracy: 53.96%\n",
            "Epoch 13/35: Train Loss: 0.6833, Train Accuracy: 52.20%\n",
            "Epoch 14/35: Train Loss: 0.6814, Train Accuracy: 54.50%\n",
            "Epoch 15/35: Train Loss: 0.6788, Train Accuracy: 56.32%\n",
            "Epoch 16/35: Train Loss: 0.6835, Train Accuracy: 53.35%\n",
            "Epoch 17/35: Train Loss: 0.6825, Train Accuracy: 54.36%\n",
            "Epoch 18/35: Train Loss: 0.6822, Train Accuracy: 53.82%\n",
            "Epoch 19/35: Train Loss: 0.6831, Train Accuracy: 53.62%\n",
            "Epoch 20/35: Train Loss: 0.6783, Train Accuracy: 55.92%\n",
            "Epoch 21/35: Train Loss: 0.6745, Train Accuracy: 54.16%\n",
            "Epoch 22/35: Train Loss: 0.6772, Train Accuracy: 54.77%\n",
            "Epoch 23/35: Train Loss: 0.6767, Train Accuracy: 54.83%\n",
            "Epoch 24/35: Train Loss: 0.6827, Train Accuracy: 54.36%\n",
            "Epoch 25/35: Train Loss: 0.6733, Train Accuracy: 54.23%\n",
            "Epoch 26/35: Train Loss: 0.6742, Train Accuracy: 55.98%\n",
            "Epoch 27/35: Train Loss: 0.6767, Train Accuracy: 55.44%\n",
            "Epoch 28/35: Train Loss: 0.6760, Train Accuracy: 56.80%\n",
            "Epoch 29/35: Train Loss: 0.6753, Train Accuracy: 55.58%\n",
            "Epoch 30/35: Train Loss: 0.6793, Train Accuracy: 54.09%\n",
            "Epoch 31/35: Train Loss: 0.6752, Train Accuracy: 56.52%\n",
            "Epoch 32/35: Train Loss: 0.6757, Train Accuracy: 55.44%\n",
            "Epoch 33/35: Train Loss: 0.6797, Train Accuracy: 55.24%\n",
            "Epoch 34/35: Train Loss: 0.6757, Train Accuracy: 55.98%\n",
            "Epoch 35/35: Train Loss: 0.6691, Train Accuracy: 57.61%\n",
            "Training complete!\n",
            "Iteration 1: Test Loss: 0.6571, Test Accuracy: 58.11%\n",
            "Iteration 2: Test Loss: 0.6571, Test Accuracy: 58.11%\n",
            "Iteration 3: Test Loss: 0.6571, Test Accuracy: 58.11%\n",
            "Iteration 4: Test Loss: 0.6571, Test Accuracy: 58.11%\n",
            "Iteration 5: Test Loss: 0.6571, Test Accuracy: 58.11%\n",
            "Iteration 6: Test Loss: 0.6571, Test Accuracy: 58.11%\n",
            "Iteration 7: Test Loss: 0.6571, Test Accuracy: 58.11%\n",
            "Iteration 8: Test Loss: 0.6571, Test Accuracy: 58.11%\n",
            "Iteration 9: Test Loss: 0.6571, Test Accuracy: 58.11%\n",
            "Iteration 10: Test Loss: 0.6571, Test Accuracy: 58.11%\n",
            "\n",
            "Average Test Loss over 10 runs: 0.6571\n",
            "Average Test Accuracy over 10 runs: 58.11%\n"
          ]
        }
      ],
      "source": [
        "model = SimplifiedVisionMambaWith80Drop(num_classes=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, device, num_epochs=35)\n",
        "\n",
        "test(model, test_loader, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop 20% patches train and test"
      ],
      "metadata": {
        "id": "IargBsbj1qwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V2_7TuhkU77y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "\n",
        "\n",
        "class SimplifiedVisionMambaWith20Drop(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        stride=16,\n",
        "        depth=12,\n",
        "        embed_dim=192,\n",
        "        num_classes=1000,\n",
        "        d_state=16,\n",
        "        drop_rate=0.1,\n",
        "        drop_path_rate=0.1,\n",
        "        drop_patch_prob=0.2,  # Probability to drop patches\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Patch Embedding with patch dropping\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            stride=stride,\n",
        "            in_chans=3,\n",
        "            embed_dim=embed_dim,\n",
        "            drop_patch_prob=drop_patch_prob,\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Positional Embeddings\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # Transformer Blocks\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    dim=embed_dim,\n",
        "                    mixer_cls=partial(SimplifiedMamba, d_state=d_state),\n",
        "                    norm_cls=nn.LayerNorm,\n",
        "                    drop_path=dpr[i],\n",
        "                )\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Final Classifier Head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize Weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.ones_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch Embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add Positional Embeddings\n",
        "        x = x + self.pos_embed[:, :x.size(1), :]  # Adjust for dropped patches\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Transformer Layers\n",
        "        for layer in self.layers:\n",
        "            x, _ = layer(x)\n",
        "\n",
        "        # Classification Head\n",
        "        x = self.norm(x[:, 0])  # CLS token output\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"2D Patch Embedding with Patch Dropping\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, stride=16, in_chans=3, embed_dim=768, drop_patch_prob=0.2):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (\n",
        "            (img_size - patch_size) // stride + 1,\n",
        "            (img_size - patch_size) // stride + 1,\n",
        "        )\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "        self.drop_patch_prob = drop_patch_prob\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans, embed_dim, kernel_size=patch_size, stride=stride\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # Convert to patches\n",
        "        x = x.flatten(2).transpose(1, 2)  # Flatten into patch tokens [B, num_patches, embed_dim]\n",
        "\n",
        "        # Drop patches probabilistically\n",
        "        if self.drop_patch_prob > 0.0 and self.training:\n",
        "            batch_size, num_patches, _ = x.size()\n",
        "            patch_mask = torch.rand(num_patches, device=x.device) > self.drop_patch_prob\n",
        "            x = x[:, patch_mask, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer Block with Mamba Mixer\"\"\"\n",
        "    def __init__(self, dim, mixer_cls, norm_cls=nn.LayerNorm, drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.mixer = mixer_cls(dim)\n",
        "        self.norm1 = norm_cls(dim)\n",
        "        self.norm2 = norm_cls(dim)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        # Self-attention with residual\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.mixer(x)\n",
        "        x = residual + self.drop_path(x)  # Add & Norm\n",
        "\n",
        "        # MLP with residual\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        return x, residual\n",
        "\n",
        "\n",
        "class SimplifiedMamba(nn.Module):\n",
        "    \"\"\"A simplified version of Mamba Mixer.\"\"\"\n",
        "    def __init__(self, dim, d_state=16):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, d_state)\n",
        "        self.act = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(d_state, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.act(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EiWYVwKdVB3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97ad4b4-a765-40f5-97f6-3daadd3606ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::rand encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 13 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::bernoulli_ encountered 11 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div_ encountered 11 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 11 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs per forward pass: 1442015232 FLOPs\n",
            "Epoch 1/35: Train Loss: 0.7010, Train Accuracy: 51.59%\n",
            "Epoch 2/35: Train Loss: 0.6862, Train Accuracy: 53.14%\n",
            "Epoch 3/35: Train Loss: 0.6800, Train Accuracy: 55.71%\n",
            "Epoch 4/35: Train Loss: 0.6847, Train Accuracy: 55.44%\n",
            "Epoch 5/35: Train Loss: 0.6798, Train Accuracy: 55.31%\n",
            "Epoch 6/35: Train Loss: 0.6709, Train Accuracy: 57.34%\n",
            "Epoch 7/35: Train Loss: 0.6722, Train Accuracy: 56.80%\n",
            "Epoch 8/35: Train Loss: 0.6747, Train Accuracy: 56.52%\n",
            "Epoch 9/35: Train Loss: 0.6735, Train Accuracy: 56.46%\n",
            "Epoch 10/35: Train Loss: 0.6629, Train Accuracy: 58.69%\n",
            "Epoch 11/35: Train Loss: 0.6625, Train Accuracy: 58.82%\n",
            "Epoch 12/35: Train Loss: 0.6639, Train Accuracy: 56.19%\n",
            "Epoch 13/35: Train Loss: 0.6770, Train Accuracy: 54.16%\n",
            "Epoch 14/35: Train Loss: 0.6683, Train Accuracy: 58.01%\n",
            "Epoch 15/35: Train Loss: 0.6644, Train Accuracy: 57.40%\n",
            "Epoch 16/35: Train Loss: 0.6637, Train Accuracy: 55.78%\n",
            "Epoch 17/35: Train Loss: 0.6561, Train Accuracy: 58.49%\n",
            "Epoch 18/35: Train Loss: 0.6586, Train Accuracy: 57.47%\n",
            "Epoch 19/35: Train Loss: 0.6496, Train Accuracy: 60.45%\n",
            "Epoch 20/35: Train Loss: 0.6551, Train Accuracy: 59.84%\n",
            "Epoch 21/35: Train Loss: 0.6539, Train Accuracy: 59.36%\n",
            "Epoch 22/35: Train Loss: 0.6450, Train Accuracy: 59.84%\n",
            "Epoch 23/35: Train Loss: 0.6436, Train Accuracy: 61.60%\n",
            "Epoch 24/35: Train Loss: 0.6501, Train Accuracy: 58.69%\n",
            "Epoch 25/35: Train Loss: 0.6448, Train Accuracy: 60.18%\n",
            "Epoch 26/35: Train Loss: 0.6389, Train Accuracy: 61.60%\n",
            "Epoch 27/35: Train Loss: 0.6496, Train Accuracy: 60.72%\n",
            "Epoch 28/35: Train Loss: 0.6325, Train Accuracy: 60.58%\n",
            "Epoch 29/35: Train Loss: 0.6328, Train Accuracy: 62.34%\n",
            "Epoch 30/35: Train Loss: 0.6407, Train Accuracy: 60.31%\n",
            "Epoch 31/35: Train Loss: 0.6320, Train Accuracy: 62.47%\n",
            "Epoch 32/35: Train Loss: 0.6322, Train Accuracy: 62.00%\n",
            "Epoch 33/35: Train Loss: 0.6323, Train Accuracy: 62.61%\n",
            "Epoch 34/35: Train Loss: 0.6385, Train Accuracy: 60.24%\n",
            "Epoch 35/35: Train Loss: 0.6380, Train Accuracy: 61.53%\n",
            "Training complete!\n",
            "Iteration 1: Test Loss: 0.6761, Test Accuracy: 57.03%\n",
            "Iteration 2: Test Loss: 0.6761, Test Accuracy: 57.03%\n",
            "Iteration 3: Test Loss: 0.6761, Test Accuracy: 57.03%\n",
            "Iteration 4: Test Loss: 0.6761, Test Accuracy: 57.03%\n",
            "Iteration 5: Test Loss: 0.6761, Test Accuracy: 57.03%\n",
            "Iteration 6: Test Loss: 0.6761, Test Accuracy: 57.03%\n",
            "Iteration 7: Test Loss: 0.6761, Test Accuracy: 57.03%\n",
            "Iteration 8: Test Loss: 0.6761, Test Accuracy: 57.03%\n",
            "Iteration 9: Test Loss: 0.6761, Test Accuracy: 57.03%\n",
            "Iteration 10: Test Loss: 0.6761, Test Accuracy: 57.03%\n",
            "\n",
            "Average Test Loss over 10 runs: 0.6761\n",
            "Average Test Accuracy over 10 runs: 57.03%\n"
          ]
        }
      ],
      "source": [
        "model = SimplifiedVisionMambaWith20Drop(num_classes=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, device, num_epochs=35)\n",
        "\n",
        "test(model, test_loader, criterion, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}